{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//*[@id=\"archive-wrapper\"]/div[4]/div[1]/div/div[1]\n",
    "//*[@id=\"archive-wrapper\"]/div[4]/div[1]/div/div[1]/div[1]/a ...link adress\n",
    "//*[@id=\"disable-copy\"]/h1...title\n",
    "//*[@id=\"disable-copy\"]/div[2]/div[1]/div/div[1]/span/a ...source\n",
    "//*[@id=\"disable-copy\"]/div[2]/div[1]/div/div[1]/p....date\n",
    "//*[@id=\"disable-copy\"]/div[2]/div[2]/div[1]/article...body \n",
    "//*[@id=\"archive-wrapper\"]/div[4]/div[1]/div/div[2]\n",
    "//*[@id=\"disable-copy\"]/h1 ...title\n",
    "//*[@id=\"disable-copy\"]/div[2]/div[1]/div/div[1]/p ...date\n",
    "//*[@id=\"disable-copy\"]/div[2]/div[2]/div[1]/article...body\n",
    "//*[@id=\"disable-copy\"]/div[2]/div[1]/div/div[1]/span/a\n",
    "//*[@id=\"archive-wrapper\"]/div[4]/div[1]/div/div[3]\n",
    "...\n",
    "//*[@id=\"archive-wrapper\"]/div[4]/div[1]/div/div[8]\n",
    "\n",
    "//*[@id=\"archive-wrapper\"]/div[5]/div/button\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Selenium WebDriver\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--headless\")  # Run in headless mode for efficiency\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Open the target website\n",
    "url = \"https://www.dealstreetasia.com/section/private-equity\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Indian PE giant Kedaara invests $350m in AI solution provider Impetus\n",
      "Source: Paramita Chatterjee\n",
      "Date: 16 January, 2025\n",
      "Article: Kedaara Capital, a homegrown private equity (PE) firm in India, has invested $350 million in Impetus Technologies, a provider of data, analytics, and AI solutions, per an announcement.\n",
      "The PE firm, led by Manish Kejriwal, has acquired a strategic stake in the company to spruce up innovation in its data and AI business.\n",
      "“The most successful business leaders today recognise data as their most valuable asset and strive to leverage it at the speed of their business, no matter where it resides,” Prav...\n",
      "--------------------------------------------------------------------------------\n",
      "Title: Gaw Capital buys 45% stake in Agility Asset Advisers to deepen presence in Japan\n",
      "Source: Stephanie Li\n",
      "Date: 16 January, 2025\n",
      "Article: Real estate private equity firm Gaw Capital Partners on Thursday said it has acquired a 45% stake in Tokyo-based Agility Asset Advisers Inc. (AAA), as the Hong Kong-headquartered firm seeks to enhance its reach in the Japanese market. ...\n",
      "--------------------------------------------------------------------------------\n",
      "Title: Indian NBFC Saarathi Finance in talks with PE firms to raise capital\n",
      "Source: Paramita Chatterjee\n",
      "Date: 16 January, 2025\n",
      "Article: Saarathi Finance, an MSME-focused NBFC in India, has initiated talks with a host of private equity firms such as Paragon Partners, TVS Capital Funds and Evolvence India, alongside impact investor Lok Capital, to raise funding as it carves out its expansion plans....\n",
      "--------------------------------------------------------------------------------\n",
      "Title: GPs' soft skills equally important amid Asia's shifting landscape, says veteran LP\n",
      "Source: Simran Vaswani\n",
      "Date: 15 January, 2025\n",
      "Article: Rather than focusing on the numbers, limited partners (LPs) are also looking for soft skills when deciding which general partners (GPs) to write cheques for, amidst a shifting investment landscape in the region, according to one of Asia’s top investors....\n",
      "--------------------------------------------------------------------------------\n",
      "Title: Oriza's semiconductor investment platform achieves first close of debut M&A fund\n",
      "Source: Stephanie Li\n",
      "Date: 15 January, 2025\n",
      "Article: Oriza Hua, the integrated circuit (IC) industry investment platform of state-owned Oriza Holding, has held the first close of its debut merger and acquisition (M&A) fund at 1.2 billion yuan ($163.7 million)....\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Wait for the page to load\n",
    "time.sleep(3)  # Adjust based on page load time\n",
    "\n",
    "# Extract article links\n",
    "articles = driver.find_elements(By.XPATH, '//*[@id=\"archive-wrapper\"]/div[4]/div[1]/div/div')\n",
    "article_links = [article.find_element(By.XPATH, './div[1]/a').get_attribute('href') for article in articles]\n",
    "\n",
    "# Loop through each article link and extract details\n",
    "for link in article_links:\n",
    "    driver.get(link)\n",
    "    time.sleep(2)  # Ensure the page loads before extracting data\n",
    "\n",
    "    try:\n",
    "        # Extract article details\n",
    "        title = driver.find_element(By.XPATH, '//*[@id=\"disable-copy\"]/h1').text\n",
    "        source = driver.find_element(By.XPATH, '//*[@id=\"disable-copy\"]/div[2]/div[1]/div/div[1]/span/a').text\n",
    "        date = driver.find_element(By.XPATH, '//*[@id=\"disable-copy\"]/div[2]/div[1]/div/div[1]/p').text\n",
    "        body = driver.find_element(By.XPATH, '//*[@id=\"disable-copy\"]/div[2]/div[2]/div[1]/article').text\n",
    "\n",
    "        # Print extracted data\n",
    "        print(f\"Title: {title}\")\n",
    "        print(f\"Source: {source}\")\n",
    "        print(f\"Date: {date}\")\n",
    "        print(f\"Article: {body[:500]}...\")  # Print first 500 characters\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting article data from {link}: {e}\")\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved article: Indian PE giant Kedaara invests $350m in AI solution provider Impetus\n",
      "Saved article: Gaw Capital buys 45% stake in Agility Asset Advisers to deepen presence in Japan\n",
      "Saved article: Indian NBFC Saarathi Finance in talks with PE firms to raise capital\n",
      "Saved article: GPs' soft skills equally important amid Asia's shifting landscape, says veteran LP\n",
      "Saved article: Oriza's semiconductor investment platform achieves first close of debut M&A fund\n",
      "\n",
      "Scraping complete! Data saved to scraped_articles.csv\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import csv\n",
    "# Set up Selenium WebDriver\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--headless\")  # Run in headless mode for efficiency\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "# Open the target website\n",
    "url = \"https://www.dealstreetasia.com/section/private-equity\"\n",
    "driver.get(url)\n",
    "\n",
    "# Wait for the page to load\n",
    "time.sleep(3)  # Adjust based on page load time\n",
    "\n",
    "# Extract article links\n",
    "articles = driver.find_elements(By.XPATH, '//*[@id=\"archive-wrapper\"]/div[4]/div[1]/div/div')\n",
    "article_links = [article.find_element(By.XPATH, './div[1]/a').get_attribute('href') for article in articles]\n",
    "\n",
    "# Create and open a CSV file for writing\n",
    "csv_filename = \"scraped_articles.csv\"\n",
    "with open(csv_filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Title\", \"Source\", \"Date\", \"Article Content\", \"URL\"])  # Added URL column\n",
    "\n",
    "    # Loop through each article link and extract details\n",
    "    for link in article_links:\n",
    "        driver.get(link)\n",
    "        time.sleep(2)  # Ensure the page loads before extracting data\n",
    "\n",
    "        try:\n",
    "            # Extract article details\n",
    "            title = driver.find_element(By.XPATH, '//*[@id=\"disable-copy\"]/h1').text.strip()\n",
    "            source = driver.find_element(By.XPATH, '//*[@id=\"disable-copy\"]/div[2]/div[1]/div/div[1]/span/a').text.strip()\n",
    "            date = driver.find_element(By.XPATH, '//*[@id=\"disable-copy\"]/div[2]/div[1]/div/div[1]/p').text.strip()\n",
    "            body = driver.find_element(By.XPATH, '//*[@id=\"disable-copy\"]/div[2]/div[2]/div[1]/article').text.strip().replace(\"\\n\", \" \")  # Remove excessive newlines\n",
    "\n",
    "            # Write the extracted data to the CSV file\n",
    "            writer.writerow([title, source, date, body, link])\n",
    "\n",
    "            print(f\"Saved article: {title}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting article data from {link}: {e}\")\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n",
    "\n",
    "print(f\"\\nScraping complete! Data saved to {csv_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'More' button clicked 1 times\n",
      "'More' button clicked 2 times\n",
      "'More' button clicked 3 times\n",
      "'More' button clicked 4 times\n",
      "'More' button clicked 5 times\n",
      "Saved article: Indian PE giant Kedaara invests $350m in AI solution provider Impetus\n",
      "Saved article: Gaw Capital buys 45% stake in Agility Asset Advisers to deepen presence in Japan\n",
      "Saved article: Indian NBFC Saarathi Finance in talks with PE firms to raise capital\n",
      "Saved article: GPs' soft skills equally important amid Asia's shifting landscape, says veteran LP\n",
      "Saved article: Oriza's semiconductor investment platform achieves first close of debut M&A fund\n",
      "Saved article: TPG said to have closed investment in Asian flower business Hasfarm Holdings\n",
      "Saved article: PAG acquires majority stake in Indian packaging firm Pravesha Industries\n",
      "Saved article: Vietnam-based PE firm ABB closes second fund at $70m\n",
      "Saved article: Bain Capital ups bid for Australia's Insignia to $1.8b, matching CC Capital\n",
      "Saved article: India’s healthcare boom lures big chunk of $14b private equity money\n",
      "Saved article: Kotak fund injects $109m into Indian IPO-hopeful Neuberg Diagnostics\n",
      "Saved article: Apollo weighs $9.5b investment in Seven & i management buyout: report\n",
      "Saved article: PE-VC investments in Indian healthcare hold steady in 2024\n",
      "Saved article: Vietnamese startup Gene Solutions ropes in SG's August Global as backer\n",
      "Saved article: Blue Earth Capital sees secondaries driving impact investments\n",
      "Saved article: Consortium offering to take over Malaysia Airports secures 84.1% stake\n",
      "Saved article: India's Botanic Healthcare bags $29m led by PE firm Skateboat Capital\n",
      "Saved article: Despite strong PE interest, Indian healthcare remains fragmented: Quadria\n",
      "Saved article: China's $47b third semiconductor 'Big Fund' kickstarts investments\n",
      "Saved article: Temasek said to have emerged as frontrunner to pick stake in India's Haldiram: Report\n",
      "Saved article: KKR said to be weighing sale of Japan's Seiyu supermarket\n",
      "Saved article: Bain Capital keen to look beyond healthcare sector in Indonesia\n",
      "Saved article: SG data centre provider Digital Edge secures $1.6b in fresh funding\n",
      "Saved article: Australia's Insignia Financial gets $1.8b takeover bid from CC Capital Partners\n",
      "Saved article: Norway sovereign wealth fund buys 45% stake in US real estate portfolio for $1b\n",
      "Saved article: India-focused PE-VC funds raise $6.8b in 2024, fail to match 2023 levels\n",
      "Saved article: Creador buys majority stake in Indonesian B2B hospitality firm MG Group\n",
      "Saved article: Mubadala acquires majority stake in UAE pharma firms\n",
      "Saved article: EQT, Temasek to sell O2 Power to JSW Neo Energy for $1.5b\n",
      "Saved article: KKR and Bain bid more than $5b each for Seven & i's non-core assets\n",
      "Saved article: Top PE deals that defined the Indian investment landscape in 2024\n",
      "Saved article: Korea's SK Inc to sell $1.86b stake in gas unit to PE firm Hahn & Co\n",
      "Saved article: Prosus to buy Latin America-focused online travel agency Despegar.com for $1.7b\n",
      "Saved article: Tamarind Health, Temasek-backed 65 Equity Partners offer to buy SGX-listed TalkMed\n",
      "Saved article: Excelsior Capital Vietnam Partners hits first close of second fund\n",
      "Saved article: Private equity firms pledge to be China-free as US tightens rules\n",
      "Saved article: Top secondaries deals in Asia that marked PE landscape in 2024\n",
      "Saved article: Creador secures $40m from FinDev Canada for sixth fund\n",
      "Saved article: After Delhivery, Multiples backs another Indian logistics startup INSTANT-XP\n",
      "Saved article: After Temasek, KKR backs Indian cloud kitchen startup Rebel Foods\n",
      "Saved article: GLP Capital Partners bags $383m for latest China income fund\n",
      "Saved article: PH Digest: Navegar exiting Royale Cold Storage; Pinnacle selling minority stake\n",
      "Saved article: GIC invests another $150m in India's TPG-backed Asia Healthcare\n",
      "Saved article: Bain's $1.7b offer rebuffed by Australia's Insignia on valuation concerns\n",
      "Saved article: Fuji Soft sticks with KKR, rejects Bain's raised buyout offer\n",
      "Saved article: Nuveen secures $100m private capital mandate from South Korean pension manager\n",
      "Saved article: DEG commits $15m to Amicus Capital's second India fund\n",
      "Saved article: Japanese cosmetics group Kose buys Lakeshore-backed skincare brand Panpuri\n",
      "Saved article: Greater China Digest: I Squared mulls offer for HKBN; Beijing Tongyizhong to buy X-Fiper\n",
      "Saved article: Five trends & strategies that will define Asia's PE landscape in 2025\n",
      "Saved article: Bain Capital offers to buy Australia's Insignia Financial for $1.7b\n",
      "Saved article: APAC may beat global markets in PE returns recovery: Preqin\n",
      "Saved article: Bain, KKR, JIP advance to next round of bidding for Seven & i's non-core ops\n",
      "Saved article: Orsted sells 50% stake in Greater Changhua 4 wind farm for $1.64bn\n",
      "Saved article: Realistic expectations, ageing sponsor portfolios to propel SE Asia buyouts\n",
      "\n",
      "Scraping complete! Data saved to more scraped_articles.csv\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import csv\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.common.exceptions import NoSuchElementException, ElementClickInterceptedException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# Set up Selenium WebDriver\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--headless\")  # Run in headless mode for efficiency\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "# Open the target website\n",
    "url = \"https://www.dealstreetasia.com/section/private-equity\"\n",
    "driver.get(url)\n",
    "\n",
    "# Wait for the page to load\n",
    "time.sleep(3)  # Adjust based on page load time\n",
    "\n",
    "# Click the \"More\" button up to 5 times to load more articles\n",
    "max_clicks = 5\n",
    "click_count = 0\n",
    "\n",
    "while click_count < max_clicks:\n",
    "    try:\n",
    "        more_button = driver.find_element(By.XPATH, '//*[@id=\"archive-wrapper\"]/div[5]/div/button')\n",
    "        driver.execute_script(\"arguments[0].click();\", more_button)  # Click using JavaScript\n",
    "        time.sleep(3)  # Wait for new articles to load\n",
    "        click_count += 1\n",
    "        print(f\"'More' button clicked {click_count} times\")\n",
    "    except (NoSuchElementException, ElementClickInterceptedException):\n",
    "        print(\"No more articles to load or button is unavailable.\")\n",
    "        break  # Stop clicking if the button is missing or can't be clicked\n",
    "\n",
    "# Extract article links after loading more content\n",
    "articles = driver.find_elements(By.XPATH, '//*[@id=\"archive-wrapper\"]/div[4]/div[1]/div/div')\n",
    "article_links = [article.find_element(By.XPATH, './div[1]/a').get_attribute('href') for article in articles]\n",
    "\n",
    "# Create and open a CSV file for writing\n",
    "csv_filename = \"more scraped_articles.csv\"\n",
    "with open(csv_filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Title\", \"Source\", \"Date\", \"Article Content\", \"URL\"])  # Added URL column\n",
    "\n",
    "    # Loop through each article link and extract details\n",
    "    for link in article_links:\n",
    "        driver.get(link)\n",
    "        time.sleep(2)  # Ensure the page loads before extracting data\n",
    "\n",
    "        try:\n",
    "            # Extract article details\n",
    "            title = driver.find_element(By.XPATH, '//*[@id=\"disable-copy\"]/h1').text.strip()\n",
    "            source = driver.find_element(By.XPATH, '//*[@id=\"disable-copy\"]/div[2]/div[1]/div/div[1]/span/a').text.strip()\n",
    "            date = driver.find_element(By.XPATH, '//*[@id=\"disable-copy\"]/div[2]/div[1]/div/div[1]/p').text.strip()\n",
    "            body = driver.find_element(By.XPATH, '//*[@id=\"disable-copy\"]/div[2]/div[2]/div[1]/article').text.strip().replace(\"\\n\", \" \")  # Remove excessive newlines\n",
    "\n",
    "            # Write the extracted data to the CSV file\n",
    "            writer.writerow([title, source, date, body, link])\n",
    "\n",
    "            print(f\"Saved article: {title}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting article data from {link}: {e}\")\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n",
    "\n",
    "print(f\"\\nScraping complete! Data saved to {csv_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: Indian PE giant Kedaara invests $350m in AI solution provider Impetus (2025-01-16)\n",
      "Saved: Gaw Capital buys 45% stake in Agility Asset Advisers to deepen presence in Japan (2025-01-16)\n",
      "Saved: Indian NBFC Saarathi Finance in talks with PE firms to raise capital (2025-01-16)\n",
      "Saved: GPs' soft skills equally important amid Asia's shifting landscape, says veteran LP (2025-01-15)\n",
      "Saved: Oriza's semiconductor investment platform achieves first close of debut M&A fund (2025-01-15)\n",
      "No more 'More' button found. Scraping complete.\n",
      "\n",
      "Scraping complete! Data saved to filtered_articles_last_2_months.csv\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import csv\n",
    "from datetime import datetime, timedelta\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.common.exceptions import NoSuchElementException, ElementClickInterceptedException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# Set up Selenium WebDriver\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--headless\")  # Run in headless mode for efficiency\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "# Open the target website\n",
    "url = \"https://www.dealstreetasia.com/section/private-equity\"\n",
    "driver.get(url)\n",
    "time.sleep(3)  \n",
    "\n",
    "# Set date threshold (2 months ago)\n",
    "two_months_ago = datetime.now() - timedelta(days=2 * 30)  # Approximate 2 months\n",
    "\n",
    "# Open CSV file for writing\n",
    "csv_filename = \"filtered_articles_last_2_months.csv\"\n",
    "with open(csv_filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Title\", \"Source\", \"Date\", \"Article Content\", \"URL\"])  # CSV Headers\n",
    "\n",
    "    stop_scraping = False  # Flag to stop when old articles appear\n",
    "\n",
    "    while not stop_scraping:\n",
    "        # Extract article links from the current page\n",
    "        articles = driver.find_elements(By.XPATH, '//*[@id=\"archive-wrapper\"]/div[4]/div[1]/div/div')\n",
    "        article_links = [article.find_element(By.XPATH, './div[1]/a').get_attribute('href') for article in articles]\n",
    "\n",
    "        # Loop through each article link and extract details\n",
    "        for link in article_links:\n",
    "            driver.get(link)\n",
    "            time.sleep(1)  \n",
    "\n",
    "            try:\n",
    "                title = driver.find_element(By.XPATH, '//*[@id=\"disable-copy\"]/h1').text.strip()\n",
    "                source = driver.find_element(By.XPATH, '//*[@id=\"disable-copy\"]/div[2]/div[1]/div/div[1]/span/a').text.strip()\n",
    "                date_text = driver.find_element(By.XPATH, '//*[@id=\"disable-copy\"]/div[2]/div[1]/div/div[1]/p').text.strip()\n",
    "                body = driver.find_element(By.XPATH, '//*[@id=\"disable-copy\"]/div[2]/div[2]/div[1]/article').text.strip().replace(\"\\n\", \" \")  \n",
    "\n",
    "                # Convert date format\n",
    "                try:\n",
    "                    article_date = datetime.strptime(date_text, \"%d %B, %Y\")\n",
    "                except ValueError:\n",
    "                    print(f\"Skipping article with invalid date format: {date_text}\")\n",
    "                    continue  \n",
    "\n",
    "                # Stop if article is older than 2 months\n",
    "                if article_date < two_months_ago:\n",
    "                    print(f\"Stopping: Found old article {title} ({article_date.strftime('%Y-%m-%d')})\")\n",
    "                    stop_scraping = True\n",
    "                    break  \n",
    "\n",
    "                # Save data to CSV\n",
    "                writer.writerow([title, source, article_date.strftime(\"%Y-%m-%d\"), body, link])\n",
    "                print(f\"Saved: {title} ({article_date.strftime('%Y-%m-%d')})\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting data from {link}: {e}\")\n",
    "\n",
    "        # If we already found an old article, stop scraping\n",
    "        if stop_scraping:\n",
    "            break  \n",
    "\n",
    "        # Click the \"More\" button to load additional articles\n",
    "        try:\n",
    "            more_button = driver.find_element(By.XPATH, '//*[@id=\"archive-wrapper\"]/div[5]/div/button')\n",
    "            driver.execute_script(\"arguments[0].click();\", more_button)  \n",
    "            time.sleep(3)  \n",
    "            print(\"Clicked 'More' button to load additional articles.\")\n",
    "        except (NoSuchElementException, ElementClickInterceptedException):\n",
    "            print(\"No more 'More' button found. Scraping complete.\")\n",
    "            break  \n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n",
    "\n",
    "print(f\"\\nScraping complete! Data saved to {csv_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import csv\n",
    "from datetime import datetime, timedelta\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.common.exceptions import NoSuchElementException, ElementClickInterceptedException, TimeoutException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Scraping articles from current page (Click count: 0)...\n",
      "\n",
      "✅ Saved: Indian PE giant Kedaara invests $350m in AI solution provider Impetus (2025-01-16)\n",
      "✅ Saved: Gaw Capital buys 45% stake in Agility Asset Advisers to deepen presence in Japan (2025-01-16)\n",
      "✅ Saved: Indian NBFC Saarathi Finance in talks with PE firms to raise capital (2025-01-16)\n",
      "✅ Saved: GPs' soft skills equally important amid Asia's shifting landscape, says veteran LP (2025-01-15)\n",
      "✅ Saved: Oriza's semiconductor investment platform achieves first close of debut M&A fund (2025-01-15)\n",
      "\n",
      "⚠️ No more 'More' button found. Stopping scraping.\n",
      "\n",
      "\n",
      "✅ Scraping complete! Data saved to 2_months.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set up Selenium WebDriver\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--headless\")  # Run headless for efficiency\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "# Open the target website\n",
    "url = \"https://www.dealstreetasia.com/section/private-equity\"\n",
    "driver.get(url)\n",
    "time.sleep(2)  # Allow page to load\n",
    "\n",
    "# Set date threshold (2 months ago)\n",
    "two_months_ago = datetime.now() - timedelta(days=2 * 30)  # Approximate 2 months\n",
    "\n",
    "\n",
    "# Set date threshold (2 months ago)\n",
    "two_months_ago = datetime.now() - timedelta(days=2 * 30)  # Approximate 2 months\n",
    "\n",
    "# Open CSV file for writing\n",
    "csv_filename = \"2_months.csv\"\n",
    "with open(csv_filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Title\", \"Source\", \"Date\", \"Article Content\", \"URL\"])  # CSV Headers\n",
    "\n",
    "    click_count = 0  # Track \"More\" button clicks\n",
    "    stop_scraping = False  # Stop when we find old articles\n",
    "\n",
    "    while click_count < 5:  # Max 5 times\n",
    "        print(f\"\\n🔍 Scraping articles from current page (Click count: {click_count})...\\n\")\n",
    "        \n",
    "        # Extract article links from the current page\n",
    "        articles = driver.find_elements(By.XPATH, '//*[@id=\"archive-wrapper\"]/div[4]/div[1]/div/div')\n",
    "        article_links = [article.find_element(By.XPATH, './div[1]/a').get_attribute('href') for article in articles]\n",
    "\n",
    "        for link in article_links:\n",
    "            driver.get(link)\n",
    "            time.sleep(1)  \n",
    "\n",
    "            try:\n",
    "                title = driver.find_element(By.XPATH, '//*[@id=\"disable-copy\"]/h1').text.strip()\n",
    "                source = driver.find_element(By.XPATH, '//*[@id=\"disable-copy\"]/div[2]/div[1]/div/div[1]/span/a').text.strip()\n",
    "                date_text = driver.find_element(By.XPATH, '//*[@id=\"disable-copy\"]/div[2]/div[1]/div/div[1]/p').text.strip()\n",
    "                body = driver.find_element(By.XPATH, '//*[@id=\"disable-copy\"]/div[2]/div[2]/div[1]/article').text.strip().replace(\"\\n\", \" \")  \n",
    "\n",
    "                # Convert date format\n",
    "                try:\n",
    "                    article_date = datetime.strptime(date_text, \"%d %B, %Y\")\n",
    "                except ValueError:\n",
    "                    print(f\"Skipping article with invalid date format: {date_text}\")\n",
    "                    continue  \n",
    "\n",
    "                # Stop if article is older than 2 months\n",
    "                if article_date < two_months_ago:\n",
    "                    print(f\"⛔ Stopping: Found old article {title} ({article_date.strftime('%Y-%m-%d')})\")\n",
    "                    stop_scraping = True\n",
    "                    break  \n",
    "\n",
    "                # Save data to CSV\n",
    "                writer.writerow([title, source, article_date.strftime(\"%Y-%m-%d\"), body, link])\n",
    "                print(f\"✅ Saved: {title} ({article_date.strftime('%Y-%m-%d')})\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Error extracting data from {link}: {e}\")\n",
    "\n",
    "            if stop_scraping:\n",
    "                break  # Stop scraping if we hit an old article\n",
    "\n",
    "        if stop_scraping:\n",
    "            break  # Stop if we reached an old article\n",
    "\n",
    "        # Try to click the \"More\" button\n",
    "        try:\n",
    "            more_button = driver.find_element(By.XPATH, '//*[@id=\"archive-wrapper\"]/div[5]/div/button')\n",
    "            driver.execute_script(\"arguments[0].click();\", more_button)  # Click using JavaScript\n",
    "            time.sleep(3)  # Wait for new articles to load\n",
    "            click_count += 1\n",
    "            print(f\"\\n🔄 'More' button clicked {click_count} times\\n\")\n",
    "\n",
    "        except (NoSuchElementException, ElementClickInterceptedException):\n",
    "            print(\"\\n⚠️ No more 'More' button found. Stopping scraping.\\n\")\n",
    "            break  # Stop if the button is missing or can't be clicked\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n",
    "\n",
    "print(f\"\\n✅ Scraping complete! Data saved to {csv_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<selenium.webdriver.chrome.options.Options at 0x252c9e1c4a0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up Selenium WebDriver\n",
    "#options = \n",
    "webdriver.ChromeOptions()\n",
    "#options.add_argument(\"--headless\")  # Run headless for efficiency\n",
    "#driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome() \n",
    "website=\"https://www.dealstreetasia.com/section/private-equity\"  \n",
    "driver.get(website)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Open the target website\n",
    "# url = \"https://www.dealstreetasia.com/section/private-equity\"\n",
    "# driver.get(url)\n",
    "# time.sleep(2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//*[@id=\"archive-wrapper\"]/div[5]/div"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
