{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "import csv\n",
    "from datetime import datetime, timedelta\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.common.exceptions import NoSuchElementException, ElementClickInterceptedException, TimeoutException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "website=\"https://www.dealstreetasia.com/section/venture-capital\"  \n",
    "driver.get(website)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//*[@id=\"archive-wrapper\"]/div[4]/div[1]/div/div[1]/div[1]/a\n",
    "//*[@id=\"archive-wrapper\"]/div[4]/div[1]/div/div[1]\n",
    "//*[@id=\"disable-copy\"]/h1\n",
    "//*[@id=\"disable-copy\"]/div[2]/div[1]/div/div[1]/span\n",
    "//*[@id=\"disable-copy\"]/div[2]/div[1]/div/div[1]/p\n",
    "//*[@id=\"disable-copy\"]/div[2]/div[2]/div[1]/article\n",
    "\n",
    "//*[@id=\"archive-wrapper\"]/div[4]/div[1]/div/div[2]\n",
    "//*[@id=\"archive-wrapper\"]/div[4]/div[1]/div/div[2]/div[1]/a\n",
    "\n",
    "# //*[@id=\"archive-wrapper\"]/div[4]/div[1]/div/div[1]\n",
    "# //*[@id=\"archive-wrapper\"]/div[4]/div[1]/div/div[1]/div[1]/a ...link adress\n",
    "# //*[@id=\"disable-copy\"]/h1...title\n",
    "# //*[@id=\"disable-copy\"]/div[2]/div[1]/div/div[1]/span/a ...source\n",
    "# //*[@id=\"disable-copy\"]/div[2]/div[1]/div/div[1]/p....date\n",
    "# //*[@id=\"disable-copy\"]/div[2]/div[2]/div[1]/article...body \n",
    "# //*[@id=\"archive-wrapper\"]/div[4]/div[1]/div/div[2]\n",
    "# //*[@id=\"disable-copy\"]/h1 ...title\n",
    "# //*[@id=\"disable-copy\"]/div[2]/div[1]/div/div[1]/p ...date\n",
    "# //*[@id=\"disable-copy\"]/div[2]/div[2]/div[1]/article...body\n",
    "# //*[@id=\"disable-copy\"]/div[2]/div[1]/div/div[1]/span/a\n",
    "# //*[@id=\"archive-wrapper\"]/div[4]/div[1]/div/div[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check the first page that the give path are properly work or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: India's Cornerstone Ventures hits first close of second fund at around $40m\n",
      "Source: Vibhuti Sharma\n",
      "Date: 15 January, 2025\n",
      "Article: Indian SaaS-focused venture capital firm Cornerstone Ventures on Wednesday said it has hit the first close of its $200-million second investment vehicle at around $40 million.\n",
      "The vehicle, launched in April last year, raised capital from domestic investors, including HNIs, family offices, corporates, and other institutions.\n",
      "Founded by former Reliance executives Rajiv Vaishnav and Abhishek Prasad, Cornerstone expects to make the final close of the fund by December 2025.\n",
      "Cornerstone raised $50 mil...\n",
      "--------------------------------------------------------------------------------\n",
      "Title: DEG commits to invest $40m in Jungle Ventures's fifth vehicle\n",
      "Source: Quynh Nguyen\n",
      "Date: 15 January, 2025\n",
      "Article: DEG, the investment arm of German state-owned development bank KfW, has committed to investing $40 million in Jungle Ventures‘s fifth fund, which has a targeted corpus of $500 million, according to a disclosure....\n",
      "--------------------------------------------------------------------------------\n",
      "Title: Indonesian fintech startup Skor Technologies secures $6.2m funding led by Argor Capital\n",
      "Source: Marsya Nabila\n",
      "Date: 14 January, 2025\n",
      "Article: Skor Technologies, the parent company of Indonesia-based startup Skorlife and Skorcard credit card, has raised $6.2 million in a pre-Series A funding round, led by Southeast Asia-focused venture capital firm Argor Capital, according to an announcement on Tuesday....\n",
      "--------------------------------------------------------------------------------\n",
      "Title: Indonesian AI startup Bythen secures $5m funding led by Vector Inc, Skystar Capital\n",
      "Source: Marsya Nabila\n",
      "Date: 14 January, 2025\n",
      "Article: Bythen, an Indonesian AI startup for virtual creators, has announced a $5-million seed financing round led by Japan’s Vector Inc. and Indonesia’s VC Skystar Capital, according to an announcement on Tuesday....\n",
      "--------------------------------------------------------------------------------\n",
      "Title: India's Boba tea and Korean fusion food brand Boba Bhai bags funding\n",
      "Source: Vibhuti Sharma\n",
      "Date: 13 January, 2025\n",
      "Article: India’s boba tea and Korean fusion food brand Boba Bhai on Monday said it has raised Rs 30 crore ($3.5 million) in a Series A round led by 8i Ventures, aiming to expand to new cities and capture the Gen Z and Millennial market.\n",
      "The round also saw participation from Titan Capital Winners Fund, Global Growth Capital, DEVC and existing investors.\n",
      "Launched in October 2023, Boba Bhai claims to process over 80,000 monthly orders and establish a presence in 42 outlets across nine cities, including Beng...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Wait for the page to load\n",
    "time.sleep(3)  # Adjust based on page load time\n",
    "\n",
    "# Extract article links\n",
    "articles = driver.find_elements(By.XPATH, '//*[@id=\"archive-wrapper\"]/div[4]/div[1]/div/div')\n",
    "article_links = [article.find_element(By.XPATH, './div[1]/a').get_attribute('href') for article in articles]\n",
    "\n",
    "# Loop through each article link and extract details\n",
    "for link in article_links:\n",
    "    driver.get(link)\n",
    "    time.sleep(4)  # Ensure the page loads before extracting data\n",
    "\n",
    "    try:\n",
    "        # Extract article details\n",
    "        title = driver.find_element(By.XPATH, '//*[@id=\"disable-copy\"]/h1').text\n",
    "        source = driver.find_element(By.XPATH, '//*[@id=\"disable-copy\"]/div[2]/div[1]/div/div[1]/span/a').text\n",
    "        date = driver.find_element(By.XPATH, '//*[@id=\"disable-copy\"]/div[2]/div[1]/div/div[1]/p').text\n",
    "        body = driver.find_element(By.XPATH, '//*[@id=\"disable-copy\"]/div[2]/div[2]/div[1]/article').text\n",
    "\n",
    "        # Print extracted data\n",
    "        print(f\"Title: {title}\")\n",
    "        print(f\"Source: {source}\")\n",
    "        print(f\"Date: {date}\")\n",
    "        print(f\"Article: {body[:500]}...\")  # Print first 500 characters\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting article data from {link}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved article: India's Cornerstone Ventures hits first close of second fund at around $40m\n",
      "Saved article: DEG commits to invest $40m in Jungle Ventures's fifth vehicle\n",
      "Saved article: Indonesian fintech startup Skor Technologies secures $6.2m funding led by Argor Capital\n",
      "Saved article: Indonesian AI startup Bythen secures $5m funding led by Vector Inc, Skystar Capital\n",
      "Saved article: India's Boba tea and Korean fusion food brand Boba Bhai bags funding\n",
      "\n",
      "Scraping complete! Data saved to scraped_articles.csv\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import csv\n",
    "# Wait for the page to load\n",
    "time.sleep(3)  # Adjust based on page load time\n",
    "\n",
    "# Extract article links\n",
    "articles = driver.find_elements(By.XPATH, '//*[@id=\"archive-wrapper\"]/div[4]/div[1]/div/div')\n",
    "article_links = [article.find_element(By.XPATH, './div[1]/a').get_attribute('href') for article in articles]\n",
    "\n",
    "# Create and open a CSV file for writing\n",
    "csv_filename = \"scraped_articles.csv\"\n",
    "with open(csv_filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Title\", \"Source\", \"Date\", \"Article Content\", \"URL\"])  # Added URL column\n",
    "\n",
    "    # Loop through each article link and extract details\n",
    "    for link in article_links:\n",
    "        driver.get(link)\n",
    "        time.sleep(4)  # Ensure the page loads before extracting data\n",
    "\n",
    "        try:\n",
    "            # Extract article details\n",
    "            title = driver.find_element(By.XPATH, '//*[@id=\"disable-copy\"]/h1').text.strip()\n",
    "            source = driver.find_element(By.XPATH, '//*[@id=\"disable-copy\"]/div[2]/div[1]/div/div[1]/span/a').text.strip()\n",
    "            date = driver.find_element(By.XPATH, '//*[@id=\"disable-copy\"]/div[2]/div[1]/div/div[1]/p').text.strip()\n",
    "            body = driver.find_element(By.XPATH, '//*[@id=\"disable-copy\"]/div[2]/div[2]/div[1]/article').text.strip().replace(\"\\n\", \" \")  # Remove excessive newlines\n",
    "\n",
    "            # Write the extracted data to the CSV file\n",
    "            writer.writerow([title, source, date, body, link])\n",
    "\n",
    "            print(f\"Saved article: {title}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting article data from {link}: {e}\")\n",
    "\n",
    "print(f\"\\nScraping complete! Data saved to {csv_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 'More' button clicked 1 times\n",
      "🔄 Total articles loaded: 15\n",
      "✅ 'More' button clicked 2 times\n",
      "🔄 Total articles loaded: 25\n",
      "✅ 'More' button clicked 3 times\n",
      "🔄 Total articles loaded: 35\n",
      "\n",
      "✅ 'More' button test complete!\n"
     ]
    }
   ],
   "source": [
    "click_count = 0\n",
    "max_clicks = 3  # Maximum clicks\n",
    "\n",
    "while click_count < max_clicks:\n",
    "    try:\n",
    "        # Locate the \"More\" button\n",
    "        more_button = driver.find_element(By.XPATH, '//*[@id=\"archive-wrapper\"]/div[5]/div/button')\n",
    "        \n",
    "        # Scroll into view and click\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView(true);\", more_button)\n",
    "        time.sleep(1)\n",
    "        driver.execute_script(\"arguments[0].click();\", more_button)  # Click using JavaScript\n",
    "        time.sleep(5)  # Wait for new articles to load\n",
    "        \n",
    "        click_count += 1\n",
    "        print(f\"✅ 'More' button clicked {click_count} times\")\n",
    "        \n",
    "        # Check if new articles are loaded\n",
    "        articles = driver.find_elements(By.XPATH, '//*[@id=\"archive-wrapper\"]/div[4]/div[1]/div/div')\n",
    "        print(f\"🔄 Total articles loaded: {len(articles)}\")\n",
    "        \n",
    "        if len(articles) == 0:\n",
    "            print(\"⚠️ No new articles found after clicking. Stopping.\")\n",
    "            break\n",
    "        \n",
    "    except (NoSuchElementException, ElementClickInterceptedException):\n",
    "        print(\"⚠️ No 'More' button found or can't be clicked. Stopping.\")\n",
    "        break  # Stop if the button is missing or can't be clicked\n",
    "\n",
    "print(\"\\n✅ 'More' button test complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved article: India's Cornerstone Ventures hits first close of second fund at around $40m - 15 January, 2025\n",
      "✅ Saved article: DEG commits to invest $40m in Jungle Ventures's fifth vehicle - 15 January, 2025\n",
      "✅ Saved article: Indonesian fintech startup Skor Technologies secures $6.2m funding led by Argor Capital - 14 January, 2025\n",
      "✅ Saved article: Indonesian AI startup Bythen secures $5m funding led by Vector Inc, Skystar Capital - 14 January, 2025\n",
      "✅ Saved article: India's Boba tea and Korean fusion food brand Boba Bhai bags funding - 13 January, 2025\n",
      "✅ Saved article: India's Cornerstone Ventures hits first close of second fund at around $40m - 15 January, 2025\n",
      "✅ Saved article: DEG commits to invest $40m in Jungle Ventures's fifth vehicle - 15 January, 2025\n",
      "✅ Saved article: Indonesian fintech startup Skor Technologies secures $6.2m funding led by Argor Capital - 14 January, 2025\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m article_links \u001b[38;5;241m=\u001b[39m [article\u001b[38;5;241m.\u001b[39mfind_element(By\u001b[38;5;241m.\u001b[39mXPATH, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./div[1]/a\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mget_attribute(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m article \u001b[38;5;129;01min\u001b[39;00m articles]\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m link \u001b[38;5;129;01min\u001b[39;00m article_links:\n\u001b[1;32m---> 30\u001b[0m     \u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlink\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# Wait for article page to load\u001b[39;00m\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     34\u001b[0m         \u001b[38;5;66;03m# Extract article details\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Yibabe\\Desktop\\Scrapingproject\\venv\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:393\u001b[0m, in \u001b[0;36mWebDriver.get\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m    391\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, url: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    392\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Loads a web page in the current browser session.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 393\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGET\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43murl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Yibabe\\Desktop\\Scrapingproject\\venv\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:382\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    379\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msessionId\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[0;32m    380\u001b[0m         params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msessionId\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession_id\n\u001b[1;32m--> 382\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommand_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver_command\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[0;32m    384\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_handler\u001b[38;5;241m.\u001b[39mcheck_response(response)\n",
      "File \u001b[1;32mc:\\Users\\Yibabe\\Desktop\\Scrapingproject\\venv\\Lib\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py:404\u001b[0m, in \u001b[0;36mRemoteConnection.execute\u001b[1;34m(self, command, params)\u001b[0m\n\u001b[0;32m    402\u001b[0m trimmed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trim_large_entries(params)\n\u001b[0;32m    403\u001b[0m LOGGER\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, command_info[\u001b[38;5;241m0\u001b[39m], url, \u001b[38;5;28mstr\u001b[39m(trimmed))\n\u001b[1;32m--> 404\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand_info\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Yibabe\\Desktop\\Scrapingproject\\venv\\Lib\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py:428\u001b[0m, in \u001b[0;36mRemoteConnection._request\u001b[1;34m(self, method, url, body)\u001b[0m\n\u001b[0;32m    425\u001b[0m     body \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    427\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client_config\u001b[38;5;241m.\u001b[39mkeep_alive:\n\u001b[1;32m--> 428\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    429\u001b[0m     statuscode \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus\n\u001b[0;32m    430\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Yibabe\\Desktop\\Scrapingproject\\venv\\Lib\\site-packages\\urllib3\\_request_methods.py:143\u001b[0m, in \u001b[0;36mRequestMethods.request\u001b[1;34m(self, method, url, body, fields, headers, json, **urlopen_kw)\u001b[0m\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_encode_url(\n\u001b[0;32m    136\u001b[0m         method,\n\u001b[0;32m    137\u001b[0m         url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39murlopen_kw,\n\u001b[0;32m    141\u001b[0m     )\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest_encode_body\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43murlopen_kw\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Yibabe\\Desktop\\Scrapingproject\\venv\\Lib\\site-packages\\urllib3\\_request_methods.py:278\u001b[0m, in \u001b[0;36mRequestMethods.request_encode_body\u001b[1;34m(self, method, url, fields, headers, encode_multipart, multipart_boundary, **urlopen_kw)\u001b[0m\n\u001b[0;32m    274\u001b[0m     extra_kw[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m, content_type)\n\u001b[0;32m    276\u001b[0m extra_kw\u001b[38;5;241m.\u001b[39mupdate(urlopen_kw)\n\u001b[1;32m--> 278\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_kw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Yibabe\\Desktop\\Scrapingproject\\venv\\Lib\\site-packages\\urllib3\\poolmanager.py:443\u001b[0m, in \u001b[0;36mPoolManager.urlopen\u001b[1;34m(self, method, url, redirect, **kw)\u001b[0m\n\u001b[0;32m    441\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 443\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest_uri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    445\u001b[0m redirect_location \u001b[38;5;241m=\u001b[39m redirect \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget_redirect_location()\n\u001b[0;32m    446\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m redirect_location:\n",
      "File \u001b[1;32mc:\\Users\\Yibabe\\Desktop\\Scrapingproject\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    784\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[0;32m    803\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Yibabe\\Desktop\\Scrapingproject\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py:534\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    532\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 534\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    536\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32mc:\\Users\\Yibabe\\Desktop\\Scrapingproject\\venv\\Lib\\site-packages\\urllib3\\connection.py:516\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    513\u001b[0m _shutdown \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshutdown\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    515\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[1;32m--> 516\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    519\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\http\\client.py:1428\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1427\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1428\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1429\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[0;32m   1430\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\http\\client.py:331\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 331\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[0;32m    333\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\http\\client.py:292\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 292\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[0;32m    294\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\socket.py:720\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    718\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    719\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 720\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    721\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    722\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# # Initialize WebDriver\n",
    "# driver = webdriver.Chrome()  # Change this if using a different browser\n",
    "# driver.get(\"https://www.dealstreetasia.com/section/venture-capital\")\n",
    "# time.sleep(3)  # Wait for page to load\n",
    "\n",
    "# # Define the exact date limit (2 months ago)\n",
    "# two_months_ago = datetime.now() - timedelta(days=15)\n",
    "\n",
    "# # Create CSV file for writing\n",
    "# csv_filename = \"articles.csv\"\n",
    "# with open(csv_filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "#     writer = csv.writer(file)\n",
    "#     writer.writerow([\"Title\", \"Source\", \"Date\", \"Article Content\", \"URL\"])  # CSV Header\n",
    "\n",
    "#     # Function to parse date from '14 January, 2025' format\n",
    "#     def parse_date(date_str):\n",
    "#         try:\n",
    "#             return datetime.strptime(date_str, \"%d %B, %Y\")  # Format: '14 January, 2025'\n",
    "#         except ValueError:\n",
    "#             return None  # Handle unexpected formats\n",
    "\n",
    "#     last_article_old = False  # Stop when an article older than 2 months is found\n",
    "\n",
    "#     while not last_article_old:\n",
    "#         # Step 1: Extract all article links on the current page\n",
    "#         articles = driver.find_elements(By.XPATH, '//*[@id=\"archive-wrapper\"]/div[4]/div[1]/div/div')\n",
    "#         article_links = [article.find_element(By.XPATH, './div[1]/a').get_attribute('href') for article in articles]\n",
    "\n",
    "#         for link in article_links:\n",
    "#             driver.get(link)\n",
    "#             time.sleep(2)  # Wait for article page to load\n",
    "\n",
    "#             try:\n",
    "#                 # Extract article details\n",
    "#                 title = driver.find_element(By.XPATH, '//*[@id=\"disable-copy\"]/h1').text.strip()\n",
    "#                 source = driver.find_element(By.XPATH, '//*[@id=\"disable-copy\"]/div[2]/div[1]/div/div[1]/span/a').text.strip()\n",
    "#                 date_text = driver.find_element(By.XPATH, '//*[@id=\"disable-copy\"]/div[2]/div[1]/div/div[1]/p').text.strip()\n",
    "#                 body = driver.find_element(By.XPATH, '//*[@id=\"disable-copy\"]/div[2]/div[2]/div[1]/article').text.strip().replace(\"\\n\", \" \")\n",
    "\n",
    "#                 # Convert date and compare with the 2-month limit\n",
    "#                 article_date = parse_date(date_text)\n",
    "#                 if article_date and article_date < two_months_ago:\n",
    "#                     print(f\"⏹️ Stopping: Found an article older than 2 months ({date_text})\")\n",
    "#                     last_article_old = True\n",
    "#                     break  # Stop processing further\n",
    "\n",
    "#                 # Save to CSV\n",
    "#                 writer.writerow([title, source, date_text, body, link])\n",
    "#                 print(f\"✅ Saved article: {title} - {date_text}\")\n",
    "\n",
    "#             except Exception as e:\n",
    "#                 print(f\"⚠️ Error extracting article data from {link}: {e}\")\n",
    "\n",
    "#             # Step 2: Go back to the main page to scrape the next article\n",
    "#             driver.back()\n",
    "#             time.sleep(2)  # Wait for the main page to reload\n",
    "\n",
    "#         if last_article_old:\n",
    "#             break  # Stop clicking \"More\" if we found an old article\n",
    "\n",
    "#         # Step 3: Click the \"More\" button to load more articles\n",
    "#         try:\n",
    "#             more_button = driver.find_element(By.XPATH, '//*[@id=\"archive-wrapper\"]/div[5]/div/button')\n",
    "#             driver.execute_script(\"arguments[0].scrollIntoView(true);\", more_button)\n",
    "#             time.sleep(1)\n",
    "#             driver.execute_script(\"arguments[0].click();\", more_button)\n",
    "#             time.sleep(5)  # Wait for new articles to load\n",
    "\n",
    "#         except (NoSuchElementException, ElementClickInterceptedException):\n",
    "#             print(\"⚠️ No 'More' button found or can't be clicked. Stopping.\")\n",
    "#             break  # Stop if the button is missing or can't be clicked\n",
    "\n",
    "# print(f\"\\n✅ Scraping complete! Data saved to {csv_filename}\")\n",
    "# driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved article: India's Cornerstone Ventures hits first close of second fund at around $40m - 15 ,01, 2025\n",
      "✅ Saved article: DEG commits to invest $40m in Jungle Ventures's fifth vehicle - 15 ,01, 2025\n",
      "✅ Saved article: Indonesian fintech startup Skor Technologies secures $6.2m funding led by Argor Capital - 14 ,01, 2025\n",
      "✅ Saved article: Indonesian AI startup Bythen secures $5m funding led by Vector Inc, Skystar Capital - 14 ,01, 2025\n"
     ]
    },
    {
     "ename": "WebDriverException",
     "evalue": "Message: unknown error: net::ERR_CONNECTION_RESET\n  (Session info: chrome=131.0.6778.266)\nStacktrace:\n\tGetHandleVerifier [0x00007FF61BD880D5+2992373]\n\t(No symbol) [0x00007FF61BA1BFD0]\n\t(No symbol) [0x00007FF61B8B590A]\n\t(No symbol) [0x00007FF61B8B3061]\n\t(No symbol) [0x00007FF61B8A3BD9]\n\t(No symbol) [0x00007FF61B8A592F]\n\t(No symbol) [0x00007FF61B8A3E9F]\n\t(No symbol) [0x00007FF61B8A371D]\n\t(No symbol) [0x00007FF61B8A363A]\n\t(No symbol) [0x00007FF61B8A1251]\n\t(No symbol) [0x00007FF61B8A1B1C]\n\t(No symbol) [0x00007FF61B8B89DA]\n\t(No symbol) [0x00007FF61B95038E]\n\t(No symbol) [0x00007FF61B92F36A]\n\t(No symbol) [0x00007FF61B94F584]\n\t(No symbol) [0x00007FF61B92F113]\n\t(No symbol) [0x00007FF61B8FA918]\n\t(No symbol) [0x00007FF61B8FBA81]\n\tGetHandleVerifier [0x00007FF61BDE6A2D+3379789]\n\tGetHandleVerifier [0x00007FF61BDFC32D+3468109]\n\tGetHandleVerifier [0x00007FF61BDF0043+3418211]\n\tGetHandleVerifier [0x00007FF61BB7C78B+847787]\n\t(No symbol) [0x00007FF61BA2757F]\n\t(No symbol) [0x00007FF61BA22FC4]\n\t(No symbol) [0x00007FF61BA2315D]\n\t(No symbol) [0x00007FF61BA12979]\n\tBaseThreadInitThunk [0x00007FF87D587034+20]\n\tRtlUserThreadStart [0x00007FF87DFFCEC1+33]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mWebDriverException\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 34\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(last_scraped_index, \u001b[38;5;28mlen\u001b[39m(article_links)):\n\u001b[0;32m     33\u001b[0m     link \u001b[38;5;241m=\u001b[39m article_links[i]\n\u001b[1;32m---> 34\u001b[0m     \u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlink\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# Wait for article page to load\u001b[39;00m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     38\u001b[0m         \u001b[38;5;66;03m# Extract article details\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Yibabe\\Desktop\\Scrapingproject\\venv\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:393\u001b[0m, in \u001b[0;36mWebDriver.get\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m    391\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, url: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    392\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Loads a web page in the current browser session.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 393\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGET\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43murl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Yibabe\\Desktop\\Scrapingproject\\venv\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:384\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    382\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[0;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m--> 384\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    385\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    386\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\Yibabe\\Desktop\\Scrapingproject\\venv\\Lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py:232\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    230\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    231\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[1;32m--> 232\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[1;31mWebDriverException\u001b[0m: Message: unknown error: net::ERR_CONNECTION_RESET\n  (Session info: chrome=131.0.6778.266)\nStacktrace:\n\tGetHandleVerifier [0x00007FF61BD880D5+2992373]\n\t(No symbol) [0x00007FF61BA1BFD0]\n\t(No symbol) [0x00007FF61B8B590A]\n\t(No symbol) [0x00007FF61B8B3061]\n\t(No symbol) [0x00007FF61B8A3BD9]\n\t(No symbol) [0x00007FF61B8A592F]\n\t(No symbol) [0x00007FF61B8A3E9F]\n\t(No symbol) [0x00007FF61B8A371D]\n\t(No symbol) [0x00007FF61B8A363A]\n\t(No symbol) [0x00007FF61B8A1251]\n\t(No symbol) [0x00007FF61B8A1B1C]\n\t(No symbol) [0x00007FF61B8B89DA]\n\t(No symbol) [0x00007FF61B95038E]\n\t(No symbol) [0x00007FF61B92F36A]\n\t(No symbol) [0x00007FF61B94F584]\n\t(No symbol) [0x00007FF61B92F113]\n\t(No symbol) [0x00007FF61B8FA918]\n\t(No symbol) [0x00007FF61B8FBA81]\n\tGetHandleVerifier [0x00007FF61BDE6A2D+3379789]\n\tGetHandleVerifier [0x00007FF61BDFC32D+3468109]\n\tGetHandleVerifier [0x00007FF61BDF0043+3418211]\n\tGetHandleVerifier [0x00007FF61BB7C78B+847787]\n\t(No symbol) [0x00007FF61BA2757F]\n\t(No symbol) [0x00007FF61BA22FC4]\n\t(No symbol) [0x00007FF61BA2315D]\n\t(No symbol) [0x00007FF61BA12979]\n\tBaseThreadInitThunk [0x00007FF87D587034+20]\n\tRtlUserThreadStart [0x00007FF87DFFCEC1+33]\n"
     ]
    }
   ],
   "source": [
    "# Initialize WebDriver\n",
    "driver = webdriver.Chrome()  # Change this if using a different browser\n",
    "driver.get(\"https://www.dealstreetasia.com/section/venture-capital\")\n",
    "time.sleep(3)  # Wait for page to load\n",
    "\n",
    "# Define the exact date limit (2 months ago)\n",
    "two_months_ago = datetime.now() - timedelta(days=20)\n",
    "\n",
    "# Create CSV file for writing\n",
    "csv_filename = \"sample_scraped_articles.csv\"\n",
    "with open(csv_filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Title\", \"Source\", \"Date\", \"Article Content\", \"URL\"])  # CSV Header\n",
    "\n",
    "    # Function to parse and reformat date\n",
    "    def parse_and_format_date(date_str):\n",
    "        try:\n",
    "            parsed_date = datetime.strptime(date_str, \"%d %B, %Y\")  # Convert '14 January, 2025'\n",
    "            return parsed_date.strftime(\"%d ,%m, %Y\")  # Format to '14 ,01, 2025'\n",
    "        except ValueError:\n",
    "            return None  # Handle unexpected formats\n",
    "\n",
    "    last_article_old = False  # Stop when an article older than 2 months is found\n",
    "    last_scraped_index = 0  # Keep track of last scraped article index\n",
    "\n",
    "    while not last_article_old:\n",
    "        # Step 1: Extract all current article links\n",
    "        articles = driver.find_elements(By.XPATH, '//*[@id=\"archive-wrapper\"]/div[4]/div[1]/div/div')\n",
    "        article_links = [article.find_element(By.XPATH, './div[1]/a').get_attribute('href') for article in articles]\n",
    "\n",
    "        # Start from the last scraped index (continue from where we left off)\n",
    "        for i in range(last_scraped_index, len(article_links)):\n",
    "            link = article_links[i]\n",
    "            driver.get(link)\n",
    "            time.sleep(2)  # Wait for article page to load\n",
    "\n",
    "            try:\n",
    "                # Extract article details\n",
    "                title = driver.find_element(By.XPATH, '//*[@id=\"disable-copy\"]/h1').text.strip()\n",
    "                source = driver.find_element(By.XPATH, '//*[@id=\"disable-copy\"]/div[2]/div[1]/div/div[1]/span/a').text.strip()\n",
    "                date_text = driver.find_element(By.XPATH, '//*[@id=\"disable-copy\"]/div[2]/div[1]/div/div[1]/p').text.strip()\n",
    "                body = driver.find_element(By.XPATH, '//*[@id=\"disable-copy\"]/div[2]/div[2]/div[1]/article').text.strip().replace(\"\\n\", \" \")\n",
    "\n",
    "                # Convert date and compare with the 2-month limit\n",
    "                formatted_date = parse_and_format_date(date_text)\n",
    "                article_date = datetime.strptime(formatted_date, \"%d ,%m, %Y\") if formatted_date else None\n",
    "\n",
    "                if article_date and article_date < two_months_ago:\n",
    "                    print(f\"⏹️ Stopping: Found an article older than 2 months ({formatted_date})\")\n",
    "                    last_article_old = True\n",
    "                    break  # Stop processing further\n",
    "\n",
    "                # Save to CSV\n",
    "                writer.writerow([title, source, formatted_date, body, link])\n",
    "                print(f\"✅ Saved article: {title} - {formatted_date}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\" Error extracting article data from {link}: {e}\")\n",
    "\n",
    "            # Update last scraped index\n",
    "            last_scraped_index = i + 1\n",
    "\n",
    "            # Go back to the main page to scrape the next article\n",
    "            driver.back()\n",
    "            time.sleep(2)  # Wait for the main page to reload\n",
    "\n",
    "        if last_article_old:\n",
    "            break  # Stop clicking \"More\" if we found an old article\n",
    "\n",
    "        # Step 2: Click the \"More\" button to load more articles\n",
    "        try:\n",
    "            more_button = driver.find_element(By.XPATH, '//*[@id=\"archive-wrapper\"]/div[5]/div/button')\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView(true);\", more_button)\n",
    "            time.sleep(1)\n",
    "            driver.execute_script(\"arguments[0].click();\", more_button)\n",
    "            time.sleep(5)  # Wait for new articles to load\n",
    "\n",
    "            # Update last_scraped_index to continue from where we left off\n",
    "            last_scraped_index = len(article_links)\n",
    "\n",
    "        except (NoSuchElementException, ElementClickInterceptedException):\n",
    "            print(\" No 'More' button found or can't be clicked. Stopping.\")\n",
    "            break  # Stop if the button is missing or can't be clicked\n",
    "\n",
    "print(f\"\\n Scraping complete! Data saved to {csv_filename}\")\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
